{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вробе бы дешевый прокси  \n",
    "https://www.ttproxy.com/pricing/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import time\n",
    "from random import randint\n",
    "from datetime import datetime\n",
    "import pymysql.cursors\n",
    "import threading\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Процедура по выводу progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress(count, total, suffix=''):\n",
    "    bar_len = 40\n",
    "    filled_len = int(round(bar_len * count / float(total)))\n",
    "\n",
    "    percents = round(100.0 * count / float(total), 2)\n",
    "    bar = '=' * filled_len + '-' * (bar_len - filled_len)\n",
    "\n",
    "    sys.stdout.write('[%s] %s%s ...%s\\r' % (bar, percents, '%', suffix))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_URL = 'https://www.b2b-center.ru/market/?show=archive'\n",
    "HEADERS = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '+\\\n",
    "           'Chrome/86.0.4240.111 Safari/537.36',\n",
    "           'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;'+\\\n",
    "           'q=0.8,application/signed-exchange;v=b3;q=0.9'}\n",
    "HOST = 'https://www.b2b-center.ru'\n",
    "REC_PER_PAGE = 20\n",
    "PROJECT_ID = 1\n",
    "FALL_COUNT = 5 # Кол-во неудачных соединений (запись в базе), после которого прокси не берется в работу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MYSQL_HOST='localhost'\n",
    "MYSQL_USER='proxy_user'\n",
    "MYSQL_PWD='proxy_user12345'\n",
    "MYSQL_DB='proxy_servers'\n",
    "\n",
    "# Чтение из базы\n",
    "def mysql_read(sql):\n",
    "    connection = pymysql.connect(host=MYSQL_HOST,\n",
    "                                 user=MYSQL_USER,\n",
    "                                 password=MYSQL_PWD,\n",
    "                                 db=MYSQL_DB,\n",
    "                                 charset='utf8mb4',\n",
    "                                 cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "    try:\n",
    "        with connection.cursor() as cursor:\n",
    "            cursor.execute(sql)\n",
    "            return cursor\n",
    "    finally:\n",
    "        connection.close()\n",
    "\n",
    "# Запись в базу\n",
    "def mysql_write(sql):\n",
    "    connection = pymysql.connect(host=MYSQL_HOST,\n",
    "                                 user=MYSQL_USER,\n",
    "                                 password=MYSQL_PWD,\n",
    "                                 db=MYSQL_DB,\n",
    "                                 charset='utf8mb4',\n",
    "                                 cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "    try:\n",
    "        with connection.cursor() as cursor:\n",
    "            # Create a new record\n",
    "            cursor.execute(sql)\n",
    "        # connection is not autocommit by default. So you must commit to save\n",
    "        # your changes.\n",
    "        connection.commit()\n",
    "\n",
    "    finally:\n",
    "        connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MYSQL_HOST2='localhost'\n",
    "MYSQL_USER2='proxy_user'\n",
    "MYSQL_PWD2='proxy_user12345'\n",
    "MYSQL_DB2='b2b_center'\n",
    "\n",
    "# Чтение из базы\n",
    "def mysql_read2(sql, values=None):\n",
    "    connection = pymysql.connect(host=MYSQL_HOST2,\n",
    "                                 user=MYSQL_USER2,\n",
    "                                 password=MYSQL_PWD2,\n",
    "                                 db=MYSQL_DB2,\n",
    "                                 charset='utf8mb4')\n",
    "\n",
    "    try:\n",
    "        with connection.cursor() as cursor:\n",
    "            if values:\n",
    "                cursor.execute(sql, values)\n",
    "            else:\n",
    "                cursor.execute(sql)\n",
    "            \n",
    "            return cursor\n",
    "    finally:\n",
    "        connection.close()\n",
    "\n",
    "# Запись в базу\n",
    "def mysql_write2(sql, values=None):\n",
    "    connection = pymysql.connect(host=MYSQL_HOST2,\n",
    "                                 user=MYSQL_USER2,\n",
    "                                 password=MYSQL_PWD2,\n",
    "                                 db=MYSQL_DB2,\n",
    "                                 charset='utf8mb4',\n",
    "                                 cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "    try:\n",
    "        with connection.cursor() as cursor:\n",
    "            # Create a new record\n",
    "            if values:\n",
    "                cursor.execute(sql, values)\n",
    "            else:\n",
    "                cursor.execute(sql)\n",
    "        # connection is not autocommit by default. So you must commit to save\n",
    "        # your changes.\n",
    "        connection.commit()\n",
    "        return cursor\n",
    "\n",
    "    finally:\n",
    "        connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Иницилизация DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_init():\n",
    "    global df_columns\n",
    "    global df\n",
    "    df_columns=['Id','Type','Name','Sponsor','Link','Start_date','End_date']\n",
    "    df = pd.DataFrame(columns=df_columns)\n",
    "    # df['Id'] = df['Id'].astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция по заполнению ДатаФрейма данными из страницы со списком закупок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_list_page(html):\n",
    "    soup = BeautifulSoup(html.text, 'html.parser')\n",
    "    search_table = soup.find_all('table', class_='table table-hover table-filled search-results')\n",
    "\n",
    "    links = search_table[0].tbody.find_all('a')\n",
    "    dates = search_table[0].tbody.find_all('td', class_='nowrap')\n",
    "\n",
    "    for i in range(0, len(links), 2):\n",
    "        auc_id = links[i].text.split('\\n')[0].split('№')[1].strip() # Номер торгов\n",
    "        print(auc_id)\n",
    "        if not len(df[df['Id'] == auc_id]):\n",
    "            auc_type = links[i].text.split('\\n')[0].split('№')[0].strip() # Вид торгов\n",
    "            auc_name = links[i].text.split('\\n')[1].strip() # if len(links[i].text.split('\\n')) > 1 else '11' # Название торгов\n",
    "            auc_name = np.nan if len(auc_name) == 0 else auc_name\n",
    "            sponsor = np.nan if len(links[i+1].text.strip()) == 0 else links[i+1].text.strip() # Наименование органзатора\n",
    "            auc_link = HOST + links[i].get('href').split('#')[0] # Ссылка на страницу торгов\n",
    "            start_date = dates[i].text # Колонка \"Опубликовано\"\n",
    "            end_date = dates[i+1].text # Колонка \"Актуально до\"\n",
    "            add_list = [auc_id, auc_type, auc_name, sponsor, auc_link, start_date, end_date]\n",
    "            df.loc[len(df)] = add_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prox = 'socks5://183.236.164.121:1081'\n",
    "prox = 'https://95.216.10.19:3128'\n",
    "proxies = {\n",
    "  'http': prox,\n",
    "  'https': prox,\n",
    "}\n",
    "# html = get_html(START_URL, proxies=proxies)\n",
    "try:\n",
    "    html = get_html('https://www.chita.ru/', proxies=proxies, timeout=10)\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"visited\" href=\"/firms/ooo-velesstroi/35328/\" target=\"_blank\">ООО \"Велесстрой\"</a>"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(html.text, 'html.parser')\n",
    "search_table = soup.find_all('table', class_='table table-hover table-filled search-results')\n",
    "search_tr = \n",
    "\n",
    "links = search_table[0].tbody.find_all('a')\n",
    "dates = search_table[0].tbody.find_all('td', class_='nowrap')\n",
    "links[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_log(text, filename='log.txt'):\n",
    "    curtime = datetime.now().isoformat().split('.')[0].replace('T', ' ')\n",
    "    try:\n",
    "        f = open(filename, 'a')\n",
    "        f.write(curtime + ' ' + text + '\\n')\n",
    "    finally:\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция возвращает результат HTML запроса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url, proxy=None, timeout=None):\n",
    "    if proxy:\n",
    "        proxies = {\n",
    "          'http': proxy[0],\n",
    "          'https': proxy[0],\n",
    "        }\n",
    "        proxy_id = proxy[1]\n",
    "        proxy_log = proxy[0]\n",
    "    else:\n",
    "        proxies = None\n",
    "        proxy_log = 'NO PROXY'\n",
    "        proxy_id = 0\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(url, headers=HEADERS, proxies=proxies, timeout=timeout)\n",
    "        return r\n",
    "    except Exception as e:\n",
    "#         write_log(proxy[0] + ' ' + str(e))\n",
    "        write_log(proxy[0] + ' Connection error')\n",
    "        mysql_write('UPDATE proxy_servers SET fall_count = fall_count + 1 WHERE id =' + str(proxy_id))\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция возвращает элемент url адреса from для последней страницы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_page(html):\n",
    "    soup = BeautifulSoup(html.text, 'html.parser')\n",
    "    search_pagination = soup.find_all('li', class_='pagi-item')\n",
    "    return int(search_pagination[-1].a.get('href').split('from=')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выдает свободный proxy из базы и отмечает его как использованный"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_me_proxy():\n",
    "    # Возвращает список из адреса прокси и id в базе, либо 0, если подходящий прокси не нашелся\n",
    "    min_time = 120 # Таймаут в секундах, после которого прокси можно использовать снова\n",
    "    fail_count = 10\n",
    "    timeout = round(time.time()) - min_time\n",
    "    query = f\"\"\"\n",
    "    SELECT ps.id as id,\n",
    "           CONCAT(pt.type, \"://\", ps.ip, \":\", ps.port) as ip\n",
    "           FROM proxy_servers ps\n",
    "    LEFT JOIN used_ip ui ON ps.id = ui.proxy_server_id\n",
    "    INNER JOIN proxy_type pt ON ps.proxy_type_id = pt.id\n",
    "    WHERE (ui.project_id = {PROJECT_ID} AND ui.timestamp < {timeout} OR ui.timestamp IS NULL) AND ps.fall_count < {fail_count}\n",
    "    ORDER BY ps.fall_count, ui.timestamp\n",
    "    LIMIT 1;\n",
    "    \"\"\"\n",
    "    cursor = mysql_read(query)\n",
    "    if cursor.rowcount == 0: # Если не нашелся \"свободный\" прокси\n",
    "        return 0\n",
    "    else:\n",
    "        result = cursor.fetchone()\n",
    "        ip = result['ip']\n",
    "        ip_id = result['id']\n",
    "        if mysql_read(f'SELECT 1 FROM used_ip WHERE project_id = {PROJECT_ID} AND proxy_server_id = {ip_id}').rowcount == 0:\n",
    "            query = \\\n",
    "            f'INSERT INTO used_ip (proxy_server_id, project_id, timestamp) VALUES ({ip_id}, {PROJECT_ID}, {timeout + min_time})'\n",
    "        else:\n",
    "            query = \\\n",
    "            f'UPDATE used_ip SET timestamp = {timeout + min_time} WHERE proxy_server_id = {ip_id} AND project_id = {PROJECT_ID}'\n",
    "#         print(query)\n",
    "        mysql_write(query)\n",
    "        return [ip, ip_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 50min 39s=====================] 100.0% ...129980 \n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ip = '\\\n",
    "131.255.239.38:3128'\n",
    "prox = 'https://' + ip\n",
    "# prox = 'socks4://' + ip\n",
    "# prox = 'socks5://' + ip\n",
    "\n",
    "\n",
    "#df_init()\n",
    "# html = get_html(START_URL, proxies=proxies)\n",
    "\n",
    "start_page = 124900\n",
    "last_page = 130000\n",
    "\n",
    "for i in range(start_page, last_page, REC_PER_PAGE):\n",
    "    curl = START_URL + '&from=' + str(i)\n",
    "    html = False\n",
    "    while not html:\n",
    "        proxy = 0\n",
    "        while not proxy:\n",
    "            proxy = give_me_proxy()\n",
    "            if not proxy:\n",
    "                write_log('No avaliable proxy')\n",
    "                sleep(10)\n",
    "        html = get_html(curl, proxy, 8)\n",
    "        if html and BeautifulSoup(html.text, 'html.parser').html.\\\n",
    "                                    text.find('Превышен максимальный лимит') > -1: # Если забанили ip\n",
    "            html = False\n",
    "            timeout = round(time.time()) + 86460 # Не используем этот прокси следующие сутки + 1 мин.\n",
    "            query = \\\n",
    "            f'UPDATE used_ip SET timestamp = {timeout} WHERE proxy_server_id = {proxy[1]} AND project_id = {PROJECT_ID}'\n",
    "            write_log(proxy[0] + ' BANNED ON THE SERVER')\n",
    "            mysql_write(query)\n",
    "    parse_list_page(html)\n",
    "    mysql_write('UPDATE proxy_servers SET fall_count = 0 WHERE id =' + str(proxy[1]))\n",
    "    progress(i - start_page + REC_PER_PAGE, last_page - start_page, suffix= str(i) + ' ')\n",
    "#     sleep(randint(60, 120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info()\n",
    "#df['Sponsor'].value_counts()\n",
    "count = str(len(df['Id'].values)) + '_' + datetime.now().isoformat().split('.')[0].replace(':', '-').replace('T', '_')\n",
    "# df.to_excel(f'first_{count}.xlsx')\n",
    "df[['Id', 'Type', 'Name', 'Sponsor', 'Start_date', 'End_date']].to_excel(f'first_{count}.xlsx')\n",
    "#df.to_pickle(f'first_{count}.pkl')\n",
    "#df.to_csv(f'first_{count}.csv', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST ZONE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_me_proxy_mt(nothing=''):\n",
    "    # Возвращает список из адреса прокси и id в базе, либо 0, если подходящий прокси не нашелся\n",
    "    min_time = 120 # Таймаут в секундах, после которого прокси можно использовать снова\n",
    "    fail_count = 10\n",
    "    timeout = round(time.time()) - min_time\n",
    "    query = f\"\"\"\n",
    "    SELECT ps.id as id,\n",
    "           CONCAT(pt.type, \"://\", ps.ip, \":\", ps.port) as ip\n",
    "           FROM proxy_servers ps\n",
    "    LEFT JOIN used_ip ui ON ps.id = ui.proxy_server_id\n",
    "    INNER JOIN proxy_type pt ON ps.proxy_type_id = pt.id\n",
    "    WHERE (ui.project_id = {PROJECT_ID} AND ui.timestamp < {timeout} OR ui.timestamp IS NULL) AND ps.fall_count < {fail_count}\n",
    "    ORDER BY ps.fall_count, ui.timestamp\n",
    "    LIMIT 1;\n",
    "    \"\"\"\n",
    "    lock.acquire()\n",
    "    cursor = mysql_read(query)\n",
    "    if cursor.rowcount == 0: # Если не нашелся \"свободный\" прокси\n",
    "        lock.release()\n",
    "        return 0\n",
    "    else:\n",
    "        result = cursor.fetchone()\n",
    "        ip = result['ip']\n",
    "        ip_id = result['id']\n",
    "        if mysql_read(f'SELECT 1 FROM used_ip WHERE project_id = {PROJECT_ID} AND proxy_server_id = {ip_id}').rowcount == 0:\n",
    "            query = \\\n",
    "            f'INSERT INTO used_ip (proxy_server_id, project_id, timestamp) VALUES ({ip_id}, {PROJECT_ID}, {timeout + min_time})'\n",
    "        else:\n",
    "            query = \\\n",
    "            f'UPDATE used_ip SET timestamp = {timeout + min_time} WHERE proxy_server_id = {ip_id} AND project_id = {PROJECT_ID}'\n",
    "#         print(query)\n",
    "        mysql_write(query)\n",
    "        lock.release()\n",
    "        return [ip, ip_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "https://www.b2b-center.ru/market/?searching=1&company_type=2&price_currency=0&date=1&date_start_dmy=01.09.2020&date_end_dmy=30.09.2020&trade=all&show=archive&from=0\n"
     ]
    }
   ],
   "source": [
    "# Возвращает список URL для этапа 0.0 (адреса из которых будут взят список ссылок для 0-го этапа)\n",
    "\n",
    "def get_url_step_0_0():\n",
    "    result = []\n",
    "    u0 = 'https://www.b2b-center.ru/market/?searching=1&company_type=2&price_currency=0&date=1'\n",
    "    u2 = '&trade=all&show=archive&from=0'\n",
    "    \n",
    "    result.append(u0 + '&date_start_dmy=01.10.2002&date_end_dmy=31.12.2007' + u2)\n",
    "    result.append(u0 + '&date_start_dmy=01.01.2008&date_end_dmy=31.12.2011' + u2)\n",
    "    for i in range(2012, 2021):\n",
    "        if 2012 <= i <= 2017:\n",
    "            result.append(u0 + f'&date_start_dmy=01.01.{i}&date_end_dmy=30.06.{i}' + u2)\n",
    "            result.append(u0 + f'&date_start_dmy=01.07.{i}&date_end_dmy=31.12.{i}' + u2)\n",
    "        else:\n",
    "            result.append(u0 + f'&date_start_dmy=01.01.{i}&date_end_dmy=30.04.{i}' + u2)\n",
    "            result.append(u0 + f'&date_start_dmy=01.05.{i}&date_end_dmy=31.08.{i}' + u2)\n",
    "            if i == 2020:\n",
    "                result.append(u0 + f'&date_start_dmy=01.09.{i}&date_end_dmy=30.09.{i}' + u2)\n",
    "            else:\n",
    "                result.append(u0 + f'&date_start_dmy=01.09.{i}&date_end_dmy=31.12.{i}' + u2)\n",
    "            \n",
    "    return(result)\n",
    "\n",
    "urls00 = get_url_step_0_0()\n",
    "print(len(urls00))\n",
    "print(urls00[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . . . . . . . . . . . . . . . . . . . . . Wall time: 6min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Добываем ссылки для 0-го этапа (ссылки на страницы по 20 заголовков конкурсов на каждой)\n",
    "step_0_urls = []\n",
    "for link in urls00:\n",
    "    html = 0\n",
    "    while not html:\n",
    "        proxy = 0\n",
    "        while not proxy:\n",
    "            proxy = give_me_proxy()\n",
    "            html = get_html(link, proxy, 8)\n",
    "            if html and BeautifulSoup(html.text, 'html.parser').html.\\\n",
    "                                        text.find('Превышен максимальный лимит') > -1: # Если забанили ip\n",
    "                html = False\n",
    "                timeout = round(time.time()) + 86460 # Не используем этот прокси следующие сутки + 1 мин.\n",
    "                query = \\\n",
    "                f'UPDATE used_ip SET timestamp = {timeout} WHERE proxy_server_id = {proxy[1]} AND project_id = {PROJECT_ID}'\n",
    "                write_log(proxy[0] + ' BANNED ON THE SERVER')\n",
    "                mysql_write(query)\n",
    "    \n",
    "    last_page = get_last_page(html)\n",
    "    for i in range(0, last_page + REC_PER_PAGE, 20):\n",
    "        step_0_urls.append(link.replace('&from=0', '&from=' + str(i)))\n",
    "    print('.', end=' ')\n",
    "    \n",
    "with open('step_0_links.txt', 'w') as file:\n",
    "    for line in step_0_urls:\n",
    "        file.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "while False:\n",
    "    # Добавляем ссылки порциями по 10000\n",
    "    rec_count = len(step_0_urls)\n",
    "    recs_per_insert = 10000\n",
    "    steps_count = rec_count // recs_per_insert\n",
    "    for i in range(steps_count):\n",
    "        insetr_values = ',\\n'.join(list(map(lambda x: '(\"' + x + '\")', step_0_urls[i*recs_per_insert:(i+1)*recs_per_insert])))\n",
    "        insert_query = 'INSERT INTO step0_urls (url) VALUES ' + insetr_values\n",
    "        mysql_write2(insert_query)\n",
    "\n",
    "    insetr_values = ',\\n'.join(list(map(lambda x: '(\"' + x + '\")', step_0_urls[(i+1)*recs_per_insert:])))\n",
    "    insert_query = 'INSERT INTO step0_urls (url) VALUES ' + insetr_values\n",
    "    mysql_write2(insert_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_mt(url, proxy=None, timeout=None):\n",
    "    if proxy:\n",
    "        proxies = {\n",
    "          'http': proxy[0],\n",
    "          'https': proxy[0],\n",
    "        }\n",
    "        proxy_id = proxy[1]\n",
    "        proxy_log = proxy[0]\n",
    "    else:\n",
    "        proxies = None\n",
    "        proxy_log = 'NO PROXY'\n",
    "        proxy_id = 0\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(url, headers=HEADERS, proxies=proxies, timeout=timeout)\n",
    "        return r\n",
    "    except Exception as e:\n",
    "        with log_lock:\n",
    "#             write_log(proxy[0] + ' ' + str(e))\n",
    "            write_log(proxy[0] + ' Connection error')\n",
    "        with lock:\n",
    "            mysql_write('UPDATE proxy_servers SET fall_count = fall_count + 1 WHERE id =' + str(proxy_id))\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ban_proxy_mt(id):\n",
    "    timeout = round(time.time()) + 86460 # Не используем этот прокси следующие сутки + 1 мин.\n",
    "    query = \\\n",
    "    f'UPDATE used_ip SET timestamp = {timeout} WHERE proxy_server_id = {id} AND project_id = {PROJECT_ID}'\n",
    "    with log_lock:\n",
    "        write_log(proxy[0] + ' BANNED ON THE SERVER')\n",
    "    with lock:\n",
    "        mysql_write(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">ПАРСИНГ MT</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_list_page_mt(html):\n",
    "    add_list = []\n",
    "    search_table = html\n",
    "    links = search_table[0].tbody.find_all('a')\n",
    "    dates = search_table[0].tbody.find_all('td', class_='nowrap')\n",
    "\n",
    "    for i in range(0, len(links), 2):\n",
    "        auc_id = links[i].text.split('\\n')[0].split('№')[1].strip() # Номер торгов\n",
    "        auc_type = links[i].text.split('\\n')[0].split('№')[0].strip() # Вид торгов\n",
    "        auc_name = links[i].text.split('\\n')[1].strip() # if len(links[i].text.split('\\n')) > 1 else '11' # Название торгов\n",
    "        auc_name = np.nan if len(auc_name) == 0 else auc_name\n",
    "        sponsor = np.nan if len(links[i+1].text.strip()) == 0 else links[i+1].text.strip() # Наименование органзатора\n",
    "        auc_link = links[i].get('href').split('#')[0] # Ссылка на страницу торгов\n",
    "        start_date = dates[i].text # Колонка \"Опубликовано\"\n",
    "        end_date = dates[i+1].text # Колонка \"Актуально до\"\n",
    "        add_list.append([auc_id, auc_type, auc_name, sponsor, auc_link, start_date, end_date])\n",
    "    return add_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_list_page_file_mt(html):\n",
    "    add_list = []\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    search_table = soup.find_all('table', class_='table table-hover table-filled search-results')\n",
    "    links = search_table[0].tbody.find_all('a')\n",
    "    dates = search_table[0].tbody.find_all('td', class_='nowrap')\n",
    "\n",
    "    for i in range(0, len(links), 2):\n",
    "        auc_id = links[i].text.split('\\n')[0].split('№')[1].strip() # Номер торгов\n",
    "        auc_type = links[i].text.split('\\n')[0].split('№')[0].strip() # Вид торгов\n",
    "        auc_name = links[i].text.split('\\n')[1].strip() # if len(links[i].text.split('\\n')) > 1 else '11' # Название торгов\n",
    "        auc_name = np.nan if len(auc_name) == 0 else auc_name\n",
    "        sponsor = np.nan if len(links[i+1].text.strip()) == 0 else links[i+1].text.strip() # Наименование органзатора\n",
    "        auc_link = links[i].get('href').split('#')[0] # Ссылка на страницу торгов\n",
    "        start_date = dates[i].text # Колонка \"Опубликовано\"\n",
    "        end_date = dates[i+1].text # Колонка \"Актуально до\"\n",
    "        add_list.append([auc_id, auc_type, auc_name, sponsor, auc_link, start_date, end_date])\n",
    "    return add_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_me_html_mt(url):\n",
    "    url_id = url[0]\n",
    "    curl = url[1]\n",
    "\n",
    "    while True:\n",
    "        proxy = give_me_proxy_mt()\n",
    "        if not proxy: # Если нет свободных прокси\n",
    "            with log_lock:\n",
    "                write_log('No avaliable proxy')\n",
    "            sleep(5)\n",
    "            continue\n",
    "        html = get_html_mt(curl, proxy, 8)\n",
    "        # Если html не вернулся или вернулся не 200-й код, начинаем заново\n",
    "        if not html or html.status_code != 200:\n",
    "            continue\n",
    "        # Если забанили ip на сервере, начинаем заново\n",
    "        if BeautifulSoup(html.text, 'html.parser').html.text.find('Превышен максимальный лимит') > -1:\n",
    "            continue\n",
    "        # Тут мы получили нормальный HTML не забаненный на сервере\n",
    "        with lock:\n",
    "            mysql_write('UPDATE proxy_servers SET fall_count = 0 WHERE id =' + str(proxy[1]))\n",
    "        soup = BeautifulSoup(html.text, 'html.parser')\n",
    "        search_table = soup.find_all('table', class_='table table-hover table-filled search-results')\n",
    "        with open('html/step01/' + str(url_id).zfill(7) +'.html', 'w', encoding='utf-8') as file:\n",
    "            file.write(str(search_table))\n",
    "        return [search_table, url_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_date(strdate):\n",
    "#   Форматирует дату под SQL формат\n",
    "    cur_date = strdate.split(' ')[0].split('.')\n",
    "    cur_time = strdate.split(' ')[1]\n",
    "    sql_date = cur_date[2] + '-' + cur_date[1] + '-' + cur_date[0] + ' ' + cur_time\n",
    "    return sql_date\n",
    "\n",
    "def insert_step1_rec(values):\n",
    "    for item in values:\n",
    "        #Проверяем есть ли в базе тендер с таким номером\n",
    "        query = 'SELECT 1 FROM tender WHERE  tender_id = %s'\n",
    "        if mysql_read2(query, item[0]).rowcount > 0:\n",
    "            query = 'SELECT 1 FROM tender WHERE  tender_id = %s AND name = %s AND start_date = %s'\n",
    "            if mysql_read2(query, (item[0], item[2], sql_date(item[5]))).rowcount > 0:\n",
    "                write_log(f'дубль тендера step1, id={item[0]} {HOST + item[4]}', 'important.log')\n",
    "                continue\n",
    "            \n",
    "        # Проверяем есть ли такой тип тендера и если нет - добавляем\n",
    "        query = f'SELECT id FROM tender_type WHERE name = %s'\n",
    "        result = mysql_read2(query, item[1])\n",
    "        if result.rowcount > 0:\n",
    "            type_id = result.fetchone()[0]\n",
    "        else:\n",
    "            query = f'INSERT INTO tender_type (name) VALUES (%s)'\n",
    "            type_id = mysql_write2(query, item[1]).lastrowid\n",
    "            \n",
    "        # Проверяем есть ли такой заказчик и если нет - добавляем\n",
    "        query = f'SELECT id FROM sponsor WHERE name = %s'\n",
    "        result = mysql_read2(query, item[3])\n",
    "        if result.rowcount > 0:\n",
    "            sponsor_id = result.fetchone()[0]\n",
    "        else:\n",
    "            query = f'INSERT INTO sponsor (name) VALUES (%s)'\n",
    "            sponsor_id = mysql_write2(query, item[3]).lastrowid\n",
    "        \n",
    "        query = 'INSERT INTO tender(tender_id, tender_type_id, name, sponsor_id, start_date, end_date, link) '\n",
    "        query += 'VALUES (%s, %s, %s, %s, %s, %s, %s)'\n",
    "        mysql_write2(query, (item[0], type_id, item[2], sponsor_id, sql_date(item[5]), sql_date(item[6]), item[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 9s-----------------------] 0.0% ...Добавляем \n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "iters = 10\n",
    "for i in range(iters):\n",
    "    # Получаем порцию ссылок для парсинга\n",
    "    rec_per_time = 100\n",
    "    query = f'SELECT id, url FROM step0_urls WHERE status = 0 ORDER BY id LIMIT {rec_per_time}'\n",
    "    s1_links = list(mysql_read2(query).fetchall())\n",
    "    \n",
    "    if len(s1_links) == 0:\n",
    "        break\n",
    "    \n",
    "    # Собираем id url и обновляем их на статус 1 (в обработке)\n",
    "    link_ids = []\n",
    "    for item in s1_links:\n",
    "        link_ids.append(str(item[0]))\n",
    "    link_ids = '(' + ', '.join(link_ids) + ')'\n",
    "    mysql_write2('UPDATE step0_urls SET status = 1 WHERE id IN ' + link_ids)\n",
    "\n",
    "    # Инициализируем блокировки\n",
    "    lock = threading.Lock()\n",
    "    log_lock = threading.Lock()\n",
    "\n",
    "    progress(i, iters, suffix= 'Скачиваем ')\n",
    "    \n",
    "    # Запускаем многопоточное скачивание\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:\n",
    "        htmls = executor.map(give_me_html_mt, s1_links)\n",
    "\n",
    "    progress(i, iters, suffix= 'Парсим ')\n",
    "    \n",
    "    # Парсим полученный результат\n",
    "    htmls = list(htmls)\n",
    "    values = []\n",
    "    for item in htmls:\n",
    "        values.extend(parse_list_page_mt(item[0]))\n",
    "\n",
    "    progress(i, iters, suffix= 'Добавляем ')\n",
    "\n",
    "    # Добавляем базу то, что напарсили\n",
    "    insert_step1_rec(values)\n",
    "\n",
    "    # Устанавливаем статус 2 (завершено) для обработанных ссылок\n",
    "    mysql_write2('UPDATE step0_urls SET status = 2 WHERE id IN ' + link_ids);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=====-----------------------------------] 11.7% ....\r"
     ]
    }
   ],
   "source": [
    "start_file = 1;\n",
    "end_file = 2300\n",
    "for i in range(start_file, end_file + 1):\n",
    "    with open('html/step01/' + str(i).zfill(7) +'.html', 'r', encoding='utf-8') as file:\n",
    "        html = file.read()\n",
    "    values = parse_list_page_file_mt(html)\n",
    "    insert_step1_rec(values)\n",
    "    progress(i - start_file + 1, end_file - start_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
